import pandas as pd
import numpy as np
import math
from scipy.stats import norm
import numpy as np
import matplotlib.pyplot as plt
import datetime
import time
import statsmodels.api as sm
import seaborn as sns
import mtm_functions

def parce_rf(St, col_names, report_date):
    St_df = pd.DataFrame(St, columns=col_names)
    melted_St = St_df.melt(id_vars=['time_delta'], var_name='risk_factor', value_name='value')
    melted_St.reset_index(drop=True, inplace=True)
    melted_St['forward_date'] = melted_St.apply(lambda row: report_date + datetime.timedelta(days=round(365 * row['time_delta'])), axis=1)
    melted_St.drop('time_delta', axis=1, inplace=True)
    return melted_St

#get deals
deals = pd.read_excel('/Users/mihailzaytsev/Desktop/CCR/IM_CCR/deals/IM_5.xlsx', engine='openpyxl')
deals['mat_date'] = pd.to_datetime(deals['mat_date'])

#get market data
data_ir_rub = pd.read_excel('/Users/mihailzaytsev/Desktop/CCR/market_data/rub_ir_10y.xlsx', engine='openpyxl')
data_ir_rub['tradedate'] = pd.to_datetime(data_ir_rub['tradedate']).dt.date
data_ir_usd = pd.read_excel('/Users/mihailzaytsev/Desktop/CCR/market_data/usd_ir_10y.xlsx', engine='openpyxl')
data_ir_usd['tradedate'] = pd.to_datetime(data_ir_usd['tradedate']).dt.date
data_fx = pd.read_excel('/Users/mihailzaytsev/Desktop/CCR/market_data/fx_rates_10y.xlsx', engine='openpyxl')
data_ir_rub = data_ir_rub.sort_values(['period', 'tradedate'])
data_ir_usd = data_ir_usd.sort_values(['period', 'tradedate'])
data_ir = pd.concat([data_ir_rub, data_ir_usd])

dates_df = set(data_ir['tradedate'])
dates_df = pd.DataFrame(dates_df, columns = ['tradedate'])
dates_df = dates_df.sort_values(['tradedate'])

period_list = sorted(list(set(data_ir['period'])))

for p in period_list: #to get risk factors in matrix form
    p_df = data_ir[data_ir['period'] == p].copy()
    p_df = p_df.rename(columns = {'value': p})
    p_df.drop(['tradetime', 'period', 'ccy', 'risk_factor'], axis= 1, inplace= True)
    dates_df = pd.merge(dates_df, p_df, how = 'inner', on = 'tradedate')

ir_df = dates_df.copy()

usd_rub = (data_fx[data_fx['secid'] == 'USDFIXME']).rename(columns = {'rate': 'USD'})
usd_rub.drop('secid', axis= 1 , inplace= True )
eur_rub = (data_fx[data_fx['secid'] == 'EURFIXME']).rename(columns = {'rate': 'EUR'})
eur_rub.drop('secid', axis= 1 , inplace= True )

fx_df = pd.merge(usd_rub, eur_rub, how = 'inner', on = 'tradedate')
fx_df['tradedate'] = pd.to_datetime(fx_df['tradedate']).dt.date
num_fx_rf = len(fx_df.columns) - 1
all_df = pd.merge(fx_df, dates_df, how = 'inner', on = 'tradedate')
report_date = all_df['tradedate'].iloc[-1]

all_df.drop('tradedate', axis= 1, inplace= True)
fx_df.drop('tradedate', axis= 1, inplace= True)
all_df_col_names = all_df.columns.tolist()
all_df_col_names.append('time_delta')

fx_log_returns = all_df.iloc[:, :num_fx_rf].apply(lambda x: (np.log(x / x.shift(1))).fillna(0), axis=0)
ir_simple_returns = all_df.iloc[:, num_fx_rf:].apply(lambda x: ((x - x.shift(1))).fillna(0), axis=0)
df_returns = pd.concat([fx_log_returns, ir_simple_returns], axis=1)

# Визуалиция распределения риск-факторов

# plt.figure(figsize=(10, 6))
# sns.histplot(df_returns['RUB_IR_1.0'], bins=300, kde=False, color='blue')
# plt.title('Гистограмма доходностей')
# plt.xlabel('Доходности')
# plt.ylabel('Частота')
# plt.show()

def volatility_rescaling(returns, wind):
    local_volatilities = returns.rolling(window=wind).std()
    local_volatilities = local_volatilities.drop(local_volatilities.index[:wind])
    returns = returns.drop(local_volatilities.index[:wind])
    T = (pd.DataFrame(local_volatilities.iloc[-1])).T
    T_parsed = pd.concat([T]*len(returns.index), ignore_index=True)
    local_volatilities = local_volatilities.reset_index(drop = True)
    returns = returns.reset_index(drop=True)
    T_parsed = T_parsed.reset_index(drop=True)
    rescaled_returns = returns/local_volatilities*T_parsed
    rescaled_returns = rescaled_returns.fillna(0.001)
    rescaled_returns.iloc[0] = 0
    return rescaled_returns

# plt.plot(df_returns['RUB_IR_0.25']) #before vola rescaling
# plt.show()

return_rescaling = True

if return_rescaling == True:
    df_returns = volatility_rescaling(df_returns, wind = 20)

# plt.plot(df_returns['RUB_IR_0.25']) #to show volatility rescaling
# plt.show()

#####################
#define necessary parametres of the model

num_sims = 100
years = 2
steps = 52 * years + 1
steps = int(steps)
num_of_risk_factors = df_returns.shape[1]
covMatrix = df_returns.cov()
corrMatrix = df_returns.corr()
dt = 1/((steps-1)/years)

#define parametres of FX risk factors
fx_log_returns.replace([np.inf, -np.inf], np.nan, inplace=True)
fx_log_returns.dropna(inplace=True)
fx_meanReturns = fx_log_returns.mean()
data = [0.075, 0.075]
index = ['USD', 'EUR']
fx_meanReturns = pd.Series(data, index=index)
fx_sdrt_div = fx_log_returns.std() * math.sqrt(5)
fx_0 = ((fx_df.iloc[-1]).to_numpy()).T

#Vasicek parametres determination
ir = all_df.iloc[:, num_fx_rf:].copy()
delta = (ir - ir.shift(1)).fillna(0)
lag = ir.shift(1).fillna(0)

parametres_df = pd.DataFrame()
for c in ir.columns.tolist():
    x = lag[c]
    y = delta[c]
    x = sm.add_constant(x)
    model = sm.OLS(y, x)
    results = model.fit()
    slope = results.params[1]  # params[1] - это slope
    intercept = results.params[0]# params[0] - это intercept
    sey = np.sqrt(results.scale)
    p_value = results.pvalues[1]
    parametres = dict({'risk_factor': c, 'a': -slope, 'b': intercept/(-slope), 'sigma': sey, 'p_value': p_value})
    parametres = pd.DataFrame([parametres])
    parametres_df = pd.concat([parametres_df, parametres])

parametres_df = parametres_df.sort_values(by=['risk_factor'])

parametres_df.set_index("risk_factor", inplace = True)

ir_a = parametres_df['a']
ir_b = parametres_df['b']
ir_sigma = parametres_df['sigma']
ir_0 = ((ir.iloc[-1] * np.exp(-ir_a * dt)).to_numpy())
ir_drift = (ir_b * (1-np.exp(-ir_a * dt)))
ir_0 = ir_0.reshape(1, -1)
fx_0 = fx_0.reshape(1, -1)

def adjust_matrix_to_positive_definite(matrix):
    eigenvalues, eigenvectors = np.linalg.eigh(matrix)
    min_positive = eigenvalues[eigenvalues > 0].min()
    epsilon = min_positive * 10
    eigenvalues[eigenvalues <= 0] = epsilon
    eigenvalues[eigenvalues <= 0.0001] = epsilon
    adjusted_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T
    eigenvalues, eigenvectors = np.linalg.eigh(adjusted_matrix)
    return adjusted_matrix

success = 0
while success == 0:
    try:
        chol = np.linalg.cholesky(corrMatrix)  # get cholezky decomposition to mind the correlation of fx and ir epsilons
        success = 1
    except:
        corrMatrix = adjust_matrix_to_positive_definite(corrMatrix)
        print('correlation Matrix was adjusted for Cholezky decomposition')

#convert to arrays
fx_mean_returns_col = fx_meanReturns.to_numpy().reshape(1, -1)
fx_sdrt_div_col = fx_sdrt_div.to_numpy().reshape(1, -1)
fx_drift = (fx_mean_returns_col - fx_sdrt_div_col ** 2 / 2) * dt
fx_drift = fx_drift.reshape(1, -1)
ir_drift_col = ir_drift.to_numpy().reshape(1, -1)
fx_drift_reshaped = (np.tile(fx_drift, (steps - 1, 1))).T
ir_drift_reshaped = (np.tile(ir_drift_col, (steps - 1, 1))).T
ir_drift = (np.tile(ir_drift, (steps - 1, 1))).T
ir_sdrt_div = np.sqrt(ir_sigma**2 * (1 - np.exp(-2 * ir_a * dt)) / (2 * ir_a)) / dt #because in (***) we will multiply by dt
fx_sdrt_div_col = fx_sdrt_div_col.reshape(1, -1)
ir_sdrt_div = ir_sdrt_div.to_numpy().reshape(1, -1)
sdrt_div = np.concatenate((fx_sdrt_div_col, ir_sdrt_div), axis=1)

n = 0
st = time.time()
St_full = np.empty([steps, num_of_risk_factors])
mtm_generated = pd.DataFrame()
while n < num_sims:
    #generate standard normal non-correlated outcomes
    uncrlted_eps = np.random.normal(size=(steps-1, df_returns.shape[1]))

    #correlate them through cholezky matrix
    crlted_eps = (chol @ uncrlted_eps.T) #абвгд

    #some adjustments
    num_columns = steps - 1 #crlted_eps.shape[1]
    sdrt_div_col_reshaped = (np.tile(sdrt_div, (num_columns, 1))).T #sdrt_div_col

    #calculation of gbm
    stochastic = crlted_eps * sdrt_div_col_reshaped * np.sqrt(dt)
    ir_stochastic = stochastic[num_fx_rf:, :].copy()
    fx_stochastic = stochastic[:num_fx_rf, :].copy()
    fx_ito_process = fx_drift_reshaped + fx_stochastic
    fx_St = (np.exp(fx_ito_process)).T
    fx_St = np.vstack([np.ones(num_fx_rf), fx_St])
    fx_St = fx_0 * fx_St.cumprod(axis=0)
    ir_St = ir_drift + ir_stochastic
    ir_St = ir_St.T
    ir_St = np.vstack([np.zeros(num_of_risk_factors - num_fx_rf), ir_St])
    ir_St = ir_0 + ir_St.cumsum(axis=0)
    St = np.concatenate((fx_St, ir_St), axis=1)

    # Define t interval correctly
    t = np.linspace(0, years, steps)
    t_column = t.reshape(-1, 1)

    # Require numpy array that is the same shape as St
    tt = np.full(shape=(num_of_risk_factors, steps), fill_value=t).T

# #риск факторы на разных осях. !!! Номера хардкодом !!!
#     plt.figure()
#
#     # Создаем первый график для первых двух столбцов St
#     ax1 = plt.gca()  # Получаем текущие оси
#     ax1.plot(tt, St[:, 0], label='USD_FX_rate')  # Первый столбец данных
#     ax1.plot(tt, St[:, 1], label='EUR_FX_rate')  # Второй столбец данных
#     ax1.set_xlabel("Время в годах $(t)$")
#     ax1.set_ylabel("Валютные риск-факторы $(S_t)$")
#
#     # Создаем вторичные оси для оставшихся 25 столбцов St
#     ax2 = ax1.twinx()  # Создаем вторичную ось Y
#     for i in range(2, 26-7):
#         ax2.plot(tt, St[:, i], label=f'IR_ {i + 1}', alpha=0.5)  # Остальные переменные с прозрачностью
#
#     ax2.set_ylabel("Процентные риск-факторы $(S_t)$")
#     # Для большей наглядности можно добавить легенду на вторичной оси, если нужно
#
#     # Добавляем название графика
#     plt.title(
#         "Реализация геометрического броуновского движения\n с разложением Холецкого"
#     )
#
#     plt.show()

    St = np.concatenate((St, t_column), axis = 1)

    St = parce_rf(St, all_df_col_names, report_date)
    St = mtm_functions.main(St, deals)
    n += 1
    print(n)
    St['nums'] = n
    mtm_generated = pd.concat([mtm_generated, St])

et = time.time()
elapsed_time = et - st
print('Execution time:', elapsed_time, 'seconds')

mtm_generatied_total_bank = mtm_generated.groupby(['forward_date', 'nums'], as_index=False).agg({'mtm': 'sum'}).reset_index()

melted_df = mtm_generatied_total_bank.pivot(index='nums', columns='forward_date', values='mtm').reset_index().melt(id_vars='nums', var_name='forward_date', value_name='mtm')
mtm_generated = mtm_generated.sort_values(by=['forward_date', 'nums'])
current_mtm = mtm_generated['mtm'].iloc[0] #чисто для диссера, только по первому клиенту

# Sort the DataFrame by 'forward_date' for proper plotting of line segments
melted_df.sort_values(by=['nums', 'forward_date'], inplace=True)

# Plotting line segments
for num in melted_df['nums'].unique():
    data_subset = melted_df[melted_df['nums'] == num]
    plt.plot(data_subset['forward_date'], data_subset['mtm'], marker='o', label=f'nums={num}')

plt.xlabel('forward_date')
plt.ylabel('mtm')
plt.title('PFE of Derivatives portfolio')
#plt.legend()
plt.grid(True)
plt.show()

#determine time tenors on which we are going to price MtM + PFE

forward_dates_list = list(set(mtm_generated['forward_date']))
forward_dates_list.sort()

valid_values = []

index_list = [1, 4, 13, 26, 52, 261, 521]

for index in index_list:
    if len(forward_dates_list) > index:
        valid_values.append(forward_dates_list[index])

mtm_generated_with_tenors = mtm_generated[mtm_generated['forward_date'].isin(valid_values)].copy()

grand_final = pd.DataFrame()
client_list = list(set(mtm_generated_with_tenors['client']))
for client in client_list:
    exp_of_the_client = mtm_generated_with_tenors[mtm_generated_with_tenors['client'] == client].copy()
    exp_of_the_client = exp_of_the_client.sort_values(by='forward_date')
    #current_mtm = exp_of_the_client['mtm'].iloc[0]
    exp_of_the_client['time_bucket'] = exp_of_the_client['forward_date'].rank(method='dense').astype(int)
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 1, 'time_bucket'] = 0.01923
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 2, 'time_bucket'] = 0.08333
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 3, 'time_bucket'] = 0.25
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 4, 'time_bucket'] = 0.5
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 5, 'time_bucket'] = 1
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 6, 'time_bucket'] = 5
    exp_of_the_client.loc[exp_of_the_client['time_bucket'] == 7, 'time_bucket'] = 10
    dates_list = list(set(exp_of_the_client['forward_date']))
    client_metrics = pd.DataFrame()
    for date in dates_list:
        exp_of_the_client_on_date = exp_of_the_client[exp_of_the_client['forward_date'] == date].copy()
        positive_exp = exp_of_the_client_on_date[exp_of_the_client_on_date['mtm'] >= 0]
        negative_exp = exp_of_the_client_on_date[exp_of_the_client_on_date['mtm'] <= 0]
        pfe = max(positive_exp['mtm'].quantile(0.99), 0)
        epe = positive_exp['mtm'].quantile(0.5)
        ene = negative_exp['mtm'].quantile(0.5)
        time_bucket = exp_of_the_client_on_date['time_bucket'].iloc[0]
        result = dict({'client': client, 'date': date, 'time_bucket': time_bucket, 'mtm': current_mtm, 'pfe': pfe - current_mtm, 'epe': epe, 'ene': ene})
        result = pd.DataFrame([result])
        client_metrics = pd.concat([client_metrics, result], ignore_index=True)
    client_metrics_gr = client_metrics.groupby(['client', 'mtm'], as_index=False).agg({'pfe': 'max', 'epe': 'max', 'ene': 'min'}).reset_index()
    print('current mtm of client', client, ' ', current_mtm)
    print('EAD=', "1.4* (", current_mtm, "+", client_metrics_gr['pfe'].iloc[0], ") =", 1.4*(current_mtm+client_metrics_gr['pfe'].iloc[0]))
    grand_final = pd.concat([grand_final, client_metrics_gr], ignore_index=True)

client_metrics.to_csv('client_metrics.csv')

print(grand_final)
